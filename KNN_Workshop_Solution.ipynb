{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95cc5941",
   "metadata": {},
   "source": [
    "## ü©∫ Dataset Selection: DermMNIST (MedMNIST v2)\n",
    "\n",
    "### What is DermMNIST?\n",
    "\n",
    "**DermMNIST** is a curated dataset of **dermatoscopic images** from the MedMNIST v2 collection, labeled by skin lesion type. The dataset includes classifications for **benign vs. malignant lesions** and features **multi-class variants**, making it ideal for medical image classification research and machine learning experiments.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Problem Statement\n",
    "\n",
    "**Research Question:**  \n",
    "Can a k-Nearest Neighbors (k-NN) classifier accurately classify dermatology lesion images in DermMNIST using similarity in feature space?\n",
    "\n",
    "**Project Objectives:**\n",
    "1. Evaluate k-NN's performance on medical image classification\n",
    "2. Compare accuracy across different values of *k* (number of neighbors)\n",
    "3. Assess performance across different feature representations (raw pixels vs. CNN embeddings)\n",
    "4. Determine if distance-based similarity aligns with diagnostic similarity in dermatology\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Why DermMNIST is ideal for k-NN:\n",
    "\n",
    "1. **üîç Distance-based similarity matches medical intuition**  \n",
    "   K-NN's core assumption ‚Äî \"similar things are close together\" ‚Äî aligns perfectly with dermatology: **similar lesions look similar**.\n",
    "\n",
    "2. **üß† Image embeddings enable meaningful comparisons**  \n",
    "   By extracting **feature embeddings** using a pretrained CNN (like ResNet), we move beyond noisy raw pixels to high-quality representations where distance truly captures visual similarity.\n",
    "\n",
    "3. **üè• Real-world medical use case**  \n",
    "   This mirrors how dermatologists work: **classifying skin lesions based on similarity to known cases** ‚Äî demonstrating k-NN's practical value in medical diagnosis support systems.\n",
    "\n",
    "4. **üìä Multi-class classification challenge**  \n",
    "   With multiple lesion types, this dataset tests k-NN's ability to handle complex, real-world medical classification beyond simple binary decisions.\n",
    "\n",
    "5. **üîì Publicly accessible and well-documented**  \n",
    "   As part of the MedMNIST v2 benchmark, the dataset is standardized and available for reproducible research.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Typical k-NN Setup for DermMNIST:\n",
    "\n",
    "**Step 1: Feature Extraction**  \n",
    "Extract features using a pretrained CNN (e.g., ResNet) to convert images into feature vectors\n",
    "\n",
    "**Step 2: k-NN Classification**  \n",
    "Run k-NN on these embeddings (not raw pixels) to classify lesions based on similarity\n",
    "\n",
    "**Step 3: Evaluation**  \n",
    "Measure how well \"visually similar\" lesions share the same diagnosis\n",
    "\n",
    "---\n",
    "\n",
    "### üéì Why This Matters:\n",
    "\n",
    "This project demonstrates that **k-NN isn't limited to toy datasets**. When paired with proper feature engineering (CNN embeddings), it becomes a powerful tool for real-world medical imaging tasks ‚Äî perfectly aligned with the workshop's goal of applying ML pipeline patterns to meaningful, complex problems in healthcare.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce29f706",
   "metadata": {},
   "source": [
    "## Why Use CNN Embeddings for This Dataset?\n",
    "\n",
    "The DermMNIST dataset consists of **medical images** of skin lesions. Unlike tabular datasets, image data is high-dimensional and contains complex visual patterns such as texture, color variation, and shape. Applying k-Nearest Neighbors (k-NN) directly to raw image pixels is generally ineffective because raw pixels do not represent visual similarity in a meaningful way.\n",
    "\n",
    "In raw pixel space, small changes in lighting, scale, or position can cause large differences in pixel values, even when two images appear visually similar. Since k-NN relies on distance calculations, this makes similarity measurements unreliable and leads to poor classification performance.\n",
    "\n",
    "To address this, we use **CNN embeddings**. A pretrained Convolutional Neural Network (CNN), such as ResNet-18, is used to transform each image into a fixed-length numerical vector (embedding) that captures high-level visual features. These embeddings encode important characteristics of the images, such as lesion structure and texture, while being more robust to low-level noise and variations.\n",
    "\n",
    "In this project, the CNN is used **only as a feature extractor**, not as a classifier. The extracted embeddings provide a more meaningful feature space in which visually similar images are closer together. The k-NN algorithm is then applied to these embeddings to perform classification based on visual similarity.\n",
    "\n",
    "Using CNN embeddings allows us to:\n",
    "- Effectively apply k-NN to image data\n",
    "- Improve distance-based similarity comparisons\n",
    "- Reduce the impact of irrelevant pixel-level variations\n",
    "- Focus the analysis on the behavior of k-NN rather than raw image representation\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Feature Extraction Process\n",
    "\n",
    "The embeddings used in this project were **pre-computed** using the `extract_embeddings_dermamnist.py` script, which implements the following pipeline:\n",
    "\n",
    "1. **Load DermMNIST dataset** ‚Äî Downloads and loads train, validation, and test splits (28√ó28 RGB images)\n",
    "2. **Initialize pretrained ResNet-18** ‚Äî Uses ImageNet-pretrained weights for robust feature extraction\n",
    "3. **Remove classification layer** ‚Äî Replaces the final fully connected layer with an identity layer, converting the model into a pure feature extractor that outputs **512-dimensional embeddings**\n",
    "4. **Process all splits** ‚Äî Extracts embeddings for all images in train, validation, and test sets\n",
    "5. **Save to disk** ‚Äî Stores all embeddings and labels in a compressed `.npz` file for efficient reuse\n",
    "\n",
    "This preprocessing step separates feature extraction from k-NN experimentation, allowing us to:\n",
    "- ‚ö° **Run k-NN experiments quickly** without re-computing embeddings each time\n",
    "- üî¨ **Focus on k-NN hyperparameter tuning** (different values of *k*)\n",
    "- üìä **Ensure reproducibility** by using the same feature representations across all experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fe6288c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets Train shape:  (7007, 512)\n",
      "datasets Val shape:  (1003, 512)\n",
      "datasets Test shape:  (2005, 512)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#getting the data\n",
    "emb_path = \"data/dermamnist_28_resnet18_embeddings.npz\" \n",
    "data = np.load(emb_path)\n",
    "\n",
    "X_train, y_train = data[\"X_train\"], data[\"y_train\"]\n",
    "X_val, y_val     = data[\"X_val\"], data[\"y_val\"]\n",
    "X_test, y_test   = data[\"X_test\"], data[\"y_test\"]\n",
    "\n",
    "print(\"datasets Train shape: \", X_train.shape)\n",
    "print(\"datasets Val shape: \", X_val.shape)\n",
    "print(\"datasets Test shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd99d53",
   "metadata": {},
   "source": [
    "## Scale the Embeddings\n",
    "\n",
    "Although CNN embeddings represent high-level image features, their individual dimensions can still have different scales. Without scaling, certain dimensions of the embedding vector may disproportionately influence the distance calculation, even if they are not more informative.\n",
    "\n",
    "To address this, we apply **feature scaling** using standardization (zero mean and unit variance). Scaling ensures that all embedding dimensions contribute more evenly to the distance computation, making similarity comparisons more meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bb4cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "X_test_s  = scaler.transform(X_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
