# üßë‚Äçüíª K-Nearest Neighbors (KNN) Workshop

## üìò Workshop Summary
This repository contains the materials and student solutions for the **K-Nearest Neighbors (KNN) Workshop**.  
The workshop introduces students to data analysis, machine learning design patterns, and active learning practices through the implementation of the KNN algorithm.

Students will explore multiple types of datasets (CSV files, APIs, relational databases), implement KNN classifiers, and design their code using the **Machine Learning Pipeline Pattern**. The workshop emphasizes collaboration through **peer programming** and reflection on algorithm performance.

---

## üéØ Learning Objectives
By completing this workshop, you will be able to:
1. Understand the intuition behind the K-Nearest Neighbors algorithm.  
2. Acquire, clean, and preprocess data from different sources.  
3. Apply KNN for classification and evaluate its performance.  
4. Architect code using the **Machine Learning Pipeline Pattern**.  
5. Collaborate with peers through active learning and peer programming.  
6. Reflect on the strengths, limitations, and real-world applications of KNN.  

---

## ü§ñ About the KNN Algorithm
The **K-Nearest Neighbors algorithm** is a simple, non-parametric machine learning method used for classification and regression.  
- A new data point is classified based on the **majority label of its *k* closest neighbors** in the training set.  
- The choice of *k* and the method of measuring distance (e.g., Euclidean, Manhattan) heavily influence performance.  
- KNN is **instance-based** (lazy learning): it stores the training data and makes predictions only at query time.  

---

## üèóÔ∏è Implementation with ML Architecture and Design Patterns
This workshop emphasizes implementing KNN using the **Machine Learning Pipeline Pattern**:
- **Data Acquisition** (from CSV, API, or relational database)  
- **Data Cleaning & Preprocessing** (scaling, encoding, handling missing values)  
- **Feature Engineering**  
- **Model Training** (KNN classifier)  
- **Evaluation** (accuracy, confusion matrix, other metrics)  
- **Reflection & Peer Comparison**  

By structuring code into **modular components**, students will learn how to build reproducible and extensible ML workflows, aligning with professional MLOps practices.

---

## üìÇ High-Level View of the Workshop
The workshop is split across two sessions (two hours total), with homework in between:

- **Session 1**  
  - Introduction to KNN (theory and Iris dataset demo)  
  - Hands-on: load and preprocess a dataset from a CSV, API, or relational DB  

- **Homework**  
  - Finalize preprocessing and prepare dataset for KNN  

- **Session 2**  
  - Implement KNN using the Pipeline Pattern  
  - Train, evaluate, and visualize results  
  - Reflect on algorithm performance  
  - Compare results with peers and discuss strengths/weaknesses of KNN  

---

## üìù Deliverables
- `KNN_Workshop_Solution.ipynb`: Your notebook implementation with code and explanations.  
- `README.md`: This file, including your name, student ID, and a brief summary of your work.  
- A PDF file with your name, student ID, and the URL to your remote repository.  


---
## üë• Team Members & Contributions

| Name | Student ID | Contribution |
|------|-----------|-------------|
| Liggia Elena Taboada Cruz | 9085905 | Data Acquisition, Cleaning & Preprocessing, Feature Engineering |
| Emmanuel Ihejiamaizu | 9080005 | Model Training (KNN hyperparameter tuning, best k selection) |
| Chao-Chung Liu | 9067679 | Evaluation (confusion matrix, classification report, per-class error) |
---

## ‚úçÔ∏è Instructor Information
- **Name:** *David Espinosa*  
- **Release:** *October 2025*  
